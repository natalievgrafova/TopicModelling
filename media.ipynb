{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import spacy\n",
    "import spacy_fastlang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongo_connection():\n",
    "\n",
    "    client = MongoClient(\"mongodb://bouman:80um4N!@ec2-15-188-255-64.eu-west-3.compute.amazonaws.com:27017/\")\n",
    "    db = client[\"media_analysis\"]\n",
    "    collection = db.articles\n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(collection):\n",
    "    articles = collection.find({}, {\"_id\": 0, \"text\": 1})\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy_fastlang.LanguageDetector at 0x7fad31655610>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"language_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Ich bin zu spÃ¤t und kan nicht mehr kommen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987383484840393"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.language_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m collection \u001b[39m=\u001b[39m mongo_connection()\n\u001b[1;32m     24\u001b[0m articles \u001b[39m=\u001b[39m get_articles(collection)\n\u001b[0;32m---> 25\u001b[0m detect_languages_and_save(articles)\n",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36mdetect_languages_and_save\u001b[0;34m(articles)\u001b[0m\n\u001b[1;32m      6\u001b[0m dutch_articles \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m article \u001b[39min\u001b[39;00m articles:\n\u001b[0;32m----> 9\u001b[0m     doc \u001b[39m=\u001b[39m nlp(article[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     10\u001b[0m     language \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39m_\u001b[39m.\u001b[39mlanguage\n\u001b[1;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m language \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfr\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/TopicModelling/venvmedia/lib/python3.11/site-packages/spacy/language.py:1030\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m   1010\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1011\u001b[0m     text: Union[\u001b[39mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     component_cfg: Optional[Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1015\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Doc:\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[39m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1030\u001b[0m     doc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_doc(text)\n\u001b[1;32m   1031\u001b[0m     \u001b[39mif\u001b[39;00m component_cfg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m         component_cfg \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/TopicModelling/venvmedia/lib/python3.11/site-packages/spacy/language.py:1121\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[39mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mstr\u001b[39m):\n\u001b[0;32m-> 1121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_doc(doc_like)\n\u001b[1;32m   1122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(doc_like, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   1123\u001b[0m     \u001b[39mreturn\u001b[39;00m Doc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\u001b[39m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m~/TopicModelling/venvmedia/lib/python3.11/site-packages/spacy/language.py:1113\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length:\n\u001b[1;32m   1110\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1111\u001b[0m         Errors\u001b[39m.\u001b[39mE088\u001b[39m.\u001b[39mformat(length\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(text), max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length)\n\u001b[1;32m   1112\u001b[0m     )\n\u001b[0;32m-> 1113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(text)\n",
      "File \u001b[0;32m~/TopicModelling/venvmedia/lib/python3.11/site-packages/spacy/tokenizer.pyx:153\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/TopicModelling/venvmedia/lib/python3.11/site-packages/spacy/tokenizer.pyx:189\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/TopicModelling/venvmedia/lib/python3.11/site-packages/spacy/tokenizer.pyx:393\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/TopicModelling/venvmedia/lib/python3.11/site-packages/spacy/tokenizer.pyx:471\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/TopicModelling/venvmedia/lib/python3.11/site-packages/spacy/vocab.pyx:161\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/TopicModelling/venvmedia/lib/python3.11/site-packages/spacy/vocab.pyx:198\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/TopicModelling/venvmedia/lib/python3.11/site-packages/spacy/lang/lex_attrs.py:144\u001b[0m, in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    140\u001b[0m             shape\u001b[39m.\u001b[39mappend(shape_char)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(shape)\n\u001b[0;32m--> 144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlower\u001b[39m(string: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m string\u001b[39m.\u001b[39mlower()\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprefix\u001b[39m(string: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def detect_languages_and_save(articles):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(\"language_detector\")\n",
    "\n",
    "    french_articles = []\n",
    "    dutch_articles = []\n",
    "\n",
    "    for article in articles:\n",
    "        doc = nlp(article[\"text\"])\n",
    "        language = doc._.language\n",
    "        if language == \"fr\":\n",
    "            french_articles.append(article[\"text\"])\n",
    "        elif language == \"nl\":\n",
    "            dutch_articles.append(article[\"text\"])\n",
    "\n",
    "    with open(\"french_articles.txt\", \"w\", encoding=\"utf-8\") as french_file:\n",
    "        french_file.write(\"\\n\".join(french_articles))\n",
    "\n",
    "    with open(\"dutch_articles.txt\", \"w\", encoding=\"utf-8\") as dutch_file:\n",
    "        dutch_file.write(\"\\n\".join(dutch_articles))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collection = mongo_connection()\n",
    "    articles = get_articles(collection)\n",
    "    detect_languages_and_save(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvmedia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
